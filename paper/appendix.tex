%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Canadian AI Latex Template    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{cai}
\captionsetup{font=small}
\begin{document}
% Editorial staff will replace the following values:
% 1. Conference Year
% 2. Issue number
% 3. Article DOI
\def\conferenceyear{2025}
\volumeheader{38}{0}%{00.000}
\begin{center}

\title{Eco Tracker Appendix}
\maketitle

\thispagestyle{empty}

% Add Authors and Affiliations in the camera ready
% for the double blind review, please leave this section as is 
\begin{tabular}{cc}
First Author\upstairs{\affilone,*}, Second Author\upstairs{\affilone}, Third Author\upstairs{\affilthree}
\\[0.25ex]
{\small \upstairs{\affilone} Affiliation One} \\
{\small \upstairs{\affiltwo} Affiliation Two} \\
{\small \upstairs{\affilthree} Affiliation Three} \\
\end{tabular}
  
% Replace with corresponding author email address
\emails{
  \upstairs{*}corresponding\_author@example.ca 
}
\vspace*{0.2in}
\end{center}

\begin{abstract}
This is an appendix of additional material that we don't have room to put into the paper.
This material will allow us to expand if needed and is here to test our own understanding of the the underlying mechanisms.


\end{abstract}

% add your keywords
\begin{keywords}{Keywords:}
Internet of Things (IoT), Reinforcement Learning, Cattle Monitoring.
\end{keywords}
\copyrightnotice

\section{Simple Battery Drain Baseline}
An initial experiment to verify that an agent is capable of learning involved a simple battery drain test.
The purpose was to check the agents' ability to learna long-term strategy.
It is simple enough to take early rewards, but in a battery drain test, the agent must learn to balance short-term rewards with long-term goals and conserve energy.
The battery was fully charged and the agent was run for as long as needed to discharge the battery while trying to maximize message collection/transmission.
The power was discretized into 400 power levels, the day was divided into 48 time steps, and the maxmimum message queue size was 5 messages and ran for 6 days until the battery was drained.

\subsection{Agent Tuning}

There were two optimizatiosn performed to tune the agent.
The first optimization involved the hyper paper tuning of alpha, gamma, epsilon, epsilon decay, $n\_steps$, episode count with rewards of 1.0 per send message and \-1.5 per lost message and 0.0001 of power multiplier.
The second optimization was a reward shaping optimization and tuned rewards for performing each action, the power multiplier, and the reward for sending a message.

The hyperparameter tuning resulted with various parameters where the agent reached a maximum message count of 230.
It would appear that the agent was not able to learn a long-term strategy and was only able to learn a short-term strategy for reward maximization.

The reward shaping optimization resulted in an agent with a maximum message count of 253.
The agent was able to learn a long-term strategy and was able to balance short-term rewards with long-term goals.
The agent appears to need intermediate rewards to learn a long-term strategy.
The followin parameters were found: power reward =0.000, Action0=-1.17324902,Action1=-0.463915495, Action2=-2, PerMessage=	0.602358605



\subsection{Battery Drain Test Results}

The agent was compared to a simple baseline static stragy of alternating transmission and collection.
The baseline strategies are as follows:
\begin{itemize}
  \item Baseline 0: Take the Transmission action every time
  \item Baseline 1: Collect once, transmit once
  \item Baseline 2: Collect twice, transmit once
  \item Baseline 3: Collect three times, transmit once
  \item Baseline 4: Collect four times, transmit once
\end{itemize}

\begin{table}[h]
  \centering
  \caption{Comparison of Baseline Models and RL Agent}
  \begin{tabular}{lcccccc}
      \toprule
      & \textbf{Baseline 0} & \textbf{Baseline 1} & \textbf{Baseline 2} & \textbf{Baseline 3} & \textbf{Baseline 4} & \textbf{Agent} \\
      \midrule
      Collection + Transmission Action & 205 & 117 & 81  & 62  & 50  & 54  \\
      Collection Action                 & 0   & 117 & 162 & 186 & 200 & 198 \\
      Total Messages                     & 205 & 234 & 243 & 248 & 250 & 252 \\
      \bottomrule
  \end{tabular}
  \label{tab:batter_drain_result}
\end{table}

The results are shown in Table \ref{tab:batter_drain_result}.
The baseline is able to send the most messages if it collects four times and sends once.
This would seem like the most optimum strategy, but the static schedule gets into trouble when the battery runs out and the entire queue of messages is lost.
The agent learns an optimal strategy of collecting four times and sending once except when the battery is low.
In this case, the agent transmits more frequently to not lose the queue of messages.

\section{Solar Baseline}
\begin{table}[h!]
  \centering
  \begin{tabular}{lcccccc}
  \hline
   & Baseline 0 & Baseline 1 & Baseline 2 & Baseline 3 & Baseline 4 & Agent \\
  \hline
  Transmission Actions & 960 & 480 & 320 & 240 & 190 & 308 \\
  Collection Actions   & 0   & 480 & 640 & 720 & 770 & 652 \\
  Sleep Actions        & 0   & 0   & 0   & 0   & 0   & 0   \\
  Avg. Message Count   & 709 & 766.5 & 769 & 782 & 805 & 790 \\
  Avg. Power Level     & 45  & 52  & 55  & 55  & 57  & 55  \\
  \hline
  \end{tabular}
  \caption{Comparison of Baselines and Agent Performance}
  \label{table:baselines_vs_agent}
  \end{table}
  

% All references should be stored in the file "references.bib".
% That call to use that file is in "cai.cls". 
% Please do not modify anything below this line.
\printbibliography[heading=subbibintoc]

\end{document}
